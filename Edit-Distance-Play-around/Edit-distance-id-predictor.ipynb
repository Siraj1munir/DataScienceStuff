{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "dataSet = pd.read_csv(\"new_topics.csv\")\n",
    "def text_process(mess):\n",
    "\tnopunc = [char for char in mess if char not in string.punctuation]\n",
    "\tnopunc = ''.join(nopunc)\n",
    "\treturn [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\t('bow', CountVectorizer(analyzer=text_process)),\n",
    "\t('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "\t('classifier', RandomForestClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "pipeline.fit(dataSet[\"Questions\"],dataSet[\"Id\"])\n",
    "# a = dataSet[\"Questions\"]\n",
    "# prediction = pipeline.predict(a)\n",
    "# print(classification_report(dataSet[\"Id\"],prediction))\n",
    "b = [\"want to check bills\"]\n",
    "prediction = pipeline.predict(b)\n",
    "print(\"Prediction:\" , prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skflow\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import datasets, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = skflow.TensorFlowLinearClassifier(n_classes=2)\n",
    "classifier.fit(X_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(X, classifier.predict(Y))\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from sklearn import metrics\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# import pandas\n",
    "# import pickle\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import skflow\n",
    "\n",
    "# ### Training data\n",
    "\n",
    "# # Download dbpedia_csv.tar.gz from\n",
    "# # https://drive.google.com/folderview?id=0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "# # Unpack: tar -xvf dbpedia_csv.tar.gz\n",
    "\n",
    "# train = pandas.read_csv('new_topics.csv', header=None)\n",
    "# X, y = train[2], train[0]\n",
    "# X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "# test = pandas.read_csv('new_topics.csv', header=None)\n",
    "# X_test, y_test = test[2], test[0]\n",
    "\n",
    "# ### Process vocabulary\n",
    "\n",
    "# MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "# vocab_processor = skflow.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "# X_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "# X_validate = np.array(list(vocab_processor.transform(X_validate)))\n",
    "# X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "# '''\n",
    "# print(\"Saving processed dataset...\")\n",
    "# if not os.path.exists('data'):\n",
    "#     os.makedirs('data')\n",
    "# pickle.dump(vocab_processor, open('data/vocab_processor', 'wb'))\n",
    "# pickle.dump(X_train, open('data/X_train', 'wb'))\n",
    "# pickle.dump(X_validate, open('data/X_validate', 'wb'))\n",
    "# pickle.dump(X_test, open('data/X_test', 'wb'))\n",
    "# pickle.dump(y_train, open('data/y_train', 'wb'))\n",
    "# pickle.dump(y_validate, open('data/y_validate', 'wb'))\n",
    "# pickle.dump(y_test, open('data/y_test', 'wb'))\n",
    "# print(\"Saving processed dataset completed\")\n",
    "# '''\n",
    "\n",
    "# '''\n",
    "# print(\"Loading processed dataset...\")\n",
    "# vocab_processor = pickle.load(open('data/vocab_processor', 'rb'))\n",
    "# X_train = pickle.load(open('data/X_train', 'rb'))\n",
    "# X_validate = pickle.load(open('data/X_validate', 'rb'))\n",
    "# X_test = pickle.load(open('data/X_test', 'rb'))\n",
    "# y_train = pickle.load(open('data/y_train', 'rb'))\n",
    "# y_validate = pickle.load(open('data/y_validate', 'rb'))\n",
    "# y_test = pickle.load(open('data/y_test', 'rb'))\n",
    "# print(\"Loading processed dataset completed\")\n",
    "# '''\n",
    "\n",
    "# n_words = len(vocab_processor.vocabulary_)\n",
    "# print('Total words: %d' % n_words)\n",
    "\n",
    "# ### Models\n",
    "\n",
    "# EMBEDDING_SIZE = 20\n",
    "# N_FILTERS = 10\n",
    "# WINDOW_SIZE = 20\n",
    "# FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\n",
    "# FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n",
    "# POOLING_WINDOW = 4\n",
    "# POOLING_STRIDE = 2\n",
    "\n",
    "# def cnn_model(X, y):\n",
    "#     \"\"\"2 layer Convolutional network to predict from sequence of words\n",
    "#     to a class.\"\"\"\n",
    "#     # Convert indexes of words into embeddings.\n",
    "#     # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "#     # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "#     # EMBEDDING_SIZE].\n",
    "#     word_vectors = skflow.ops.categorical_variable(X, n_classes=n_words,\n",
    "#         embedding_size=EMBEDDING_SIZE, name='words')\n",
    "#     word_vectors = tf.expand_dims(word_vectors, 3)\n",
    "#     with tf.variable_scope('CNN_Layer1'):\n",
    "#         # Apply Convolution filtering on input sequence.\n",
    "#         conv1 = skflow.ops.conv2d(word_vectors, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n",
    "#         # Add a RELU for non linearity.\n",
    "#         conv1 = tf.nn.relu(conv1)\n",
    "#         # Max pooling across output of Convlution+Relu.\n",
    "#         pool1 = tf.nn.max_pool(conv1, ksize=[1, POOLING_WINDOW, 1, 1], \n",
    "#             strides=[1, POOLING_STRIDE, 1, 1], padding='SAME')\n",
    "#         # Transpose matrix so that n_filters from convolution becomes width.\n",
    "#         pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "#     with tf.variable_scope('CNN_Layer2'):\n",
    "#         # Second level of convolution filtering.\n",
    "#         conv2 = skflow.ops.conv2d(pool1, N_FILTERS, FILTER_SHAPE2,\n",
    "#             padding='VALID')\n",
    "#         # Max across each filter to get useful features for classification.\n",
    "#         pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "#     # Apply regular WX + B and classification.\n",
    "#     return skflow.models.logistic_regression(pool2, y)\n",
    "\n",
    "# # classifier = skflow.TensorFlowEstimator(model_fn=cnn_model, n_classes=15, steps=1000, optimizer='Adam', learning_rate=0.003, continue_training=True)\n",
    "\n",
    "# print(\"Loading classifier...\")\n",
    "# classifier = skflow.TensorFlowEstimator.restore('data/classifier')\n",
    "# print(\"Loading classifier completed\")\n",
    "\n",
    "# classifier.learning_rate = 0.0007\n",
    "# classifier.steps=500\n",
    "\n",
    "# # Continuesly train for 1000 steps & predict on test set.\n",
    "# while True:\n",
    "#     classifier.fit(X_train, y_train, logdir='/tmp/tf_examples/word_cnn')\n",
    "#     print(\"Saving classifier...\")\n",
    "#     classifier.save('data/classifier')\n",
    "#     print(\"Saving classifier completed\")\n",
    "#     validation_score = metrics.accuracy_score(y_validate, classifier.predict(X_validate))\n",
    "#     print('Validation set accuracy: {0:f}'.format(validation_score))\n",
    "#     test_score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "#     print('Test set accuracy: {0:f}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn import Estimator\n",
    "import skflow\n",
    "### Training data\n",
    "\n",
    "# Downloads, unpacks and reads DBpedia dataset.\n",
    "\n",
    "# Downloads, unpacks and reads DBpedia dataset.\n",
    "dbpedia = learn.datasets.load_dataset('dbpedia')\n",
    "X_train, y_train = pandas.DataFrame(dbpedia.train.data)[1], pandas.Series(dbpedia.train.target)\n",
    "X_test, y_test = pandas.DataFrame(dbpedia.test.data)[1], pandas.Series(dbpedia.test.target)### Process vocabulary\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.float32(list(vocab_processor.fit_transform(X_train)))\n",
    "X_test = np.float32(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)\n",
    "\n",
    "### Models\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "def average_model(X, y):\n",
    "    word_vectors = learn.ops.categorical_variable(X, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    features = tf.reduce_max(word_vectors, reduction_indices=1)\n",
    "    return learn.models.logistic_regression(features, y)\n",
    "\n",
    "def rnn_model(X, y):\n",
    "    \"\"\"Recurrent neural network model to predict from sequence of words\n",
    "    to a class.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(X, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = learn.ops.seq2seq_inputs(1, MAX_DOCUMENT_LENGTH, word_vectors, output_length= n_classes)\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    return learn.models.logistic_regression(encoding, y)\n",
    "# steps = 1000\n",
    "# n_classes = 15\n",
    "# optimizer = 'Adam'\n",
    "# learning_rate = 0.01\n",
    "# continue_training = True\n",
    "# classifier = learn.Estimator(model_fn=rnn_model)\n",
    "feature_columns= X_train\n",
    "classifier = tf.estimator.Estimator(model_fn=rnn_model, params = {feature_columns: X_train, steps:1000 ,n_classes:15 , optimizer: 'Adam', learning_rate:0.01,continue_training:True})\n",
    "# Continuously train for 1000 steps & predict on test set.\n",
    "while True:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "    print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrphilosopher/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 35s 2us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 4s 2us/step\n",
      "> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpbertm9f2/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbd5617a5c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpbertm9f2/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31474, step = 1\n",
      "INFO:tensorflow:global_step/sec: 72.3987\n",
      "INFO:tensorflow:loss = 38.52305, step = 101 (1.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.84\n",
      "INFO:tensorflow:loss = 33.99044, step = 201 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.01\n",
      "INFO:tensorflow:loss = 25.836887, step = 301 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.883\n",
      "INFO:tensorflow:loss = 21.456955, step = 401 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.389\n",
      "INFO:tensorflow:loss = 21.625221, step = 501 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.554\n",
      "INFO:tensorflow:loss = 14.085503, step = 601 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.025\n",
      "INFO:tensorflow:loss = 17.066545, step = 701 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.343\n",
      "INFO:tensorflow:loss = 14.3464775, step = 801 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.886\n",
      "INFO:tensorflow:loss = 17.604115, step = 901 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.244\n",
      "INFO:tensorflow:loss = 11.898851, step = 1001 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.71\n",
      "INFO:tensorflow:loss = 23.731224, step = 1101 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.909\n",
      "INFO:tensorflow:loss = 21.394749, step = 1201 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.953\n",
      "INFO:tensorflow:loss = 25.642702, step = 1301 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.682\n",
      "INFO:tensorflow:loss = 16.361076, step = 1401 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.028\n",
      "INFO:tensorflow:loss = 15.764581, step = 1501 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.884\n",
      "INFO:tensorflow:loss = 9.253094, step = 1601 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.453\n",
      "INFO:tensorflow:loss = 18.813087, step = 1701 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.154\n",
      "INFO:tensorflow:loss = 11.867695, step = 1801 (0.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.163\n",
      "INFO:tensorflow:loss = 13.466562, step = 1901 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.376\n",
      "INFO:tensorflow:loss = 15.100323, step = 2001 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.246\n",
      "INFO:tensorflow:loss = 19.563236, step = 2101 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.186\n",
      "INFO:tensorflow:loss = 19.700176, step = 2201 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.525\n",
      "INFO:tensorflow:loss = 14.011733, step = 2301 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.568\n",
      "INFO:tensorflow:loss = 11.172232, step = 2401 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.633\n",
      "INFO:tensorflow:loss = 12.364228, step = 2501 (0.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.515\n",
      "INFO:tensorflow:loss = 10.627107, step = 2601 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.699\n",
      "INFO:tensorflow:loss = 17.644459, step = 2701 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.245\n",
      "INFO:tensorflow:loss = 18.299755, step = 2801 (0.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.558\n",
      "INFO:tensorflow:loss = 18.526377, step = 2901 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.295\n",
      "INFO:tensorflow:loss = 11.348265, step = 3001 (0.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.332\n",
      "INFO:tensorflow:loss = 14.0384445, step = 3101 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.2\n",
      "INFO:tensorflow:loss = 24.539907, step = 3201 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.879\n",
      "INFO:tensorflow:loss = 11.7047415, step = 3301 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.447\n",
      "INFO:tensorflow:loss = 11.20701, step = 3401 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.884\n",
      "INFO:tensorflow:loss = 8.590862, step = 3501 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.931\n",
      "INFO:tensorflow:loss = 11.8554535, step = 3601 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.533\n",
      "INFO:tensorflow:loss = 18.408388, step = 3701 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.497\n",
      "INFO:tensorflow:loss = 8.886179, step = 3801 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.799\n",
      "INFO:tensorflow:loss = 17.624477, step = 3901 (0.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.384\n",
      "INFO:tensorflow:loss = 14.332535, step = 4001 (0.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.052\n",
      "INFO:tensorflow:loss = 15.28249, step = 4101 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.253\n",
      "INFO:tensorflow:loss = 14.896688, step = 4201 (0.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.131\n",
      "INFO:tensorflow:loss = 19.530186, step = 4301 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.646\n",
      "INFO:tensorflow:loss = 10.100901, step = 4401 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.339\n",
      "INFO:tensorflow:loss = 22.652851, step = 4501 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.018\n",
      "INFO:tensorflow:loss = 12.675139, step = 4601 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.659\n",
      "INFO:tensorflow:loss = 25.273884, step = 4701 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.518\n",
      "INFO:tensorflow:loss = 19.267076, step = 4801 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.802\n",
      "INFO:tensorflow:loss = 16.188728, step = 4901 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.47\n",
      "INFO:tensorflow:loss = 13.425184, step = 5001 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.126\n",
      "INFO:tensorflow:loss = 9.901026, step = 5101 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.127\n",
      "INFO:tensorflow:loss = 19.956997, step = 5201 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.291\n",
      "INFO:tensorflow:loss = 17.80548, step = 5301 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.754\n",
      "INFO:tensorflow:loss = 16.857243, step = 5401 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.397\n",
      "INFO:tensorflow:loss = 13.670229, step = 5501 (0.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.466\n",
      "INFO:tensorflow:loss = 6.7057953, step = 5601 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.392\n",
      "INFO:tensorflow:loss = 18.842365, step = 5701 (0.551 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 171.101\n",
      "INFO:tensorflow:loss = 15.283693, step = 5801 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.408\n",
      "INFO:tensorflow:loss = 10.972188, step = 5901 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.554\n",
      "INFO:tensorflow:loss = 16.597904, step = 6001 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.737\n",
      "INFO:tensorflow:loss = 11.222596, step = 6101 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.723\n",
      "INFO:tensorflow:loss = 14.2318535, step = 6201 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.144\n",
      "INFO:tensorflow:loss = 14.547214, step = 6301 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.387\n",
      "INFO:tensorflow:loss = 9.428928, step = 6401 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.626\n",
      "INFO:tensorflow:loss = 8.499983, step = 6501 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.121\n",
      "INFO:tensorflow:loss = 12.525518, step = 6601 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.659\n",
      "INFO:tensorflow:loss = 13.517422, step = 6701 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.717\n",
      "INFO:tensorflow:loss = 2.8473258, step = 6801 (0.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.786\n",
      "INFO:tensorflow:loss = 9.970818, step = 6901 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.519\n",
      "INFO:tensorflow:loss = 8.424129, step = 7001 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.934\n",
      "INFO:tensorflow:loss = 20.607727, step = 7101 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.059\n",
      "INFO:tensorflow:loss = 23.628311, step = 7201 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.633\n",
      "INFO:tensorflow:loss = 15.719375, step = 7301 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.915\n",
      "INFO:tensorflow:loss = 11.442061, step = 7401 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.129\n",
      "INFO:tensorflow:loss = 18.284986, step = 7501 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.206\n",
      "INFO:tensorflow:loss = 19.526014, step = 7601 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.647\n",
      "INFO:tensorflow:loss = 12.143502, step = 7701 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.942\n",
      "INFO:tensorflow:loss = 18.14592, step = 7801 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.533\n",
      "INFO:tensorflow:loss = 9.505241, step = 7901 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.263\n",
      "INFO:tensorflow:loss = 6.8304124, step = 8001 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.74\n",
      "INFO:tensorflow:loss = 16.177254, step = 8101 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.68\n",
      "INFO:tensorflow:loss = 12.513477, step = 8201 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.686\n",
      "INFO:tensorflow:loss = 14.721782, step = 8301 (0.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.168\n",
      "INFO:tensorflow:loss = 14.154621, step = 8401 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.324\n",
      "INFO:tensorflow:loss = 16.068907, step = 8501 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.807\n",
      "INFO:tensorflow:loss = 17.421352, step = 8601 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.916\n",
      "INFO:tensorflow:loss = 20.628782, step = 8701 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.725\n",
      "INFO:tensorflow:loss = 15.273944, step = 8801 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.24\n",
      "INFO:tensorflow:loss = 3.3804362, step = 8901 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.301\n",
      "INFO:tensorflow:loss = 11.57864, step = 9001 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.234\n",
      "INFO:tensorflow:loss = 12.903984, step = 9101 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.376\n",
      "INFO:tensorflow:loss = 17.076477, step = 9201 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.61\n",
      "INFO:tensorflow:loss = 12.546698, step = 9301 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.203\n",
      "INFO:tensorflow:loss = 17.057495, step = 9401 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.413\n",
      "INFO:tensorflow:loss = 13.877059, step = 9501 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.878\n",
      "INFO:tensorflow:loss = 10.699108, step = 9601 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.206\n",
      "INFO:tensorflow:loss = 9.152163, step = 9701 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.939\n",
      "INFO:tensorflow:loss = 13.834261, step = 9801 (0.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.711\n",
      "INFO:tensorflow:loss = 15.364383, step = 9901 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.983\n",
      "INFO:tensorflow:loss = 13.780795, step = 10001 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.581\n",
      "INFO:tensorflow:loss = 12.735963, step = 10101 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.809\n",
      "INFO:tensorflow:loss = 14.372769, step = 10201 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.504\n",
      "INFO:tensorflow:loss = 13.112002, step = 10301 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.146\n",
      "INFO:tensorflow:loss = 23.199001, step = 10401 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.713\n",
      "INFO:tensorflow:loss = 7.6348433, step = 10501 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.796\n",
      "INFO:tensorflow:loss = 15.441022, step = 10601 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.551\n",
      "INFO:tensorflow:loss = 15.907141, step = 10701 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.547\n",
      "INFO:tensorflow:loss = 12.971424, step = 10801 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.799\n",
      "INFO:tensorflow:loss = 9.414835, step = 10901 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.213\n",
      "INFO:tensorflow:loss = 6.738323, step = 11001 (0.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.479\n",
      "INFO:tensorflow:loss = 11.415309, step = 11101 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.161\n",
      "INFO:tensorflow:loss = 12.6312275, step = 11201 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.561\n",
      "INFO:tensorflow:loss = 11.022729, step = 11301 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.95\n",
      "INFO:tensorflow:loss = 6.355901, step = 11401 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.347\n",
      "INFO:tensorflow:loss = 13.345418, step = 11501 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.241\n",
      "INFO:tensorflow:loss = 16.799906, step = 11601 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.964\n",
      "INFO:tensorflow:loss = 12.007214, step = 11701 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.088\n",
      "INFO:tensorflow:loss = 9.961568, step = 11801 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.827\n",
      "INFO:tensorflow:loss = 19.621346, step = 11901 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.817\n",
      "INFO:tensorflow:loss = 21.80939, step = 12001 (0.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.537\n",
      "INFO:tensorflow:loss = 6.231138, step = 12101 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.83\n",
      "INFO:tensorflow:loss = 14.921468, step = 12201 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.631\n",
      "INFO:tensorflow:loss = 7.566717, step = 12301 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.419\n",
      "INFO:tensorflow:loss = 13.448956, step = 12401 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.175\n",
      "INFO:tensorflow:loss = 7.835708, step = 12501 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.091\n",
      "INFO:tensorflow:loss = 9.598583, step = 12601 (0.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.788\n",
      "INFO:tensorflow:loss = 16.871466, step = 12701 (0.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.65\n",
      "INFO:tensorflow:loss = 11.855352, step = 12801 (0.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.747\n",
      "INFO:tensorflow:loss = 14.522753, step = 12901 (0.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.3\n",
      "INFO:tensorflow:loss = 11.336408, step = 13001 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.034\n",
      "INFO:tensorflow:loss = 9.240637, step = 13101 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.521\n",
      "INFO:tensorflow:loss = 13.363287, step = 13201 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.222\n",
      "INFO:tensorflow:loss = 10.543541, step = 13301 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.733\n",
      "INFO:tensorflow:loss = 14.916413, step = 13401 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.968\n",
      "INFO:tensorflow:loss = 8.290602, step = 13501 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.783\n",
      "INFO:tensorflow:loss = 16.735653, step = 13601 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.017\n",
      "INFO:tensorflow:loss = 10.488699, step = 13701 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.188\n",
      "INFO:tensorflow:loss = 9.520721, step = 13801 (0.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.696\n",
      "INFO:tensorflow:loss = 11.779905, step = 13901 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.192\n",
      "INFO:tensorflow:loss = 7.137681, step = 14001 (0.591 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 184.256\n",
      "INFO:tensorflow:loss = 12.4751835, step = 14101 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.56\n",
      "INFO:tensorflow:loss = 8.182442, step = 14201 (0.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.857\n",
      "INFO:tensorflow:loss = 15.272541, step = 14301 (0.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.108\n",
      "INFO:tensorflow:loss = 13.691511, step = 14401 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.99\n",
      "INFO:tensorflow:loss = 18.339718, step = 14501 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.577\n",
      "INFO:tensorflow:loss = 10.578041, step = 14601 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.257\n",
      "INFO:tensorflow:loss = 10.576, step = 14701 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.131\n",
      "INFO:tensorflow:loss = 10.1890955, step = 14801 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.952\n",
      "INFO:tensorflow:loss = 15.655239, step = 14901 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.776\n",
      "INFO:tensorflow:loss = 8.595922, step = 15001 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.312\n",
      "INFO:tensorflow:loss = 10.834646, step = 15101 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.739\n",
      "INFO:tensorflow:loss = 12.185104, step = 15201 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.305\n",
      "INFO:tensorflow:loss = 11.196738, step = 15301 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.37\n",
      "INFO:tensorflow:loss = 10.399986, step = 15401 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.666\n",
      "INFO:tensorflow:loss = 12.501652, step = 15501 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.74\n",
      "INFO:tensorflow:loss = 6.454541, step = 15601 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.978\n",
      "INFO:tensorflow:loss = 9.336747, step = 15701 (0.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.684\n",
      "INFO:tensorflow:loss = 18.030794, step = 15801 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.436\n",
      "INFO:tensorflow:loss = 6.5197597, step = 15901 (0.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.557\n",
      "INFO:tensorflow:loss = 10.238261, step = 16001 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.818\n",
      "INFO:tensorflow:loss = 11.230848, step = 16101 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.77\n",
      "INFO:tensorflow:loss = 19.329983, step = 16201 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.187\n",
      "INFO:tensorflow:loss = 12.4090805, step = 16301 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.438\n",
      "INFO:tensorflow:loss = 7.3400126, step = 16401 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.909\n",
      "INFO:tensorflow:loss = 8.988432, step = 16501 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.39\n",
      "INFO:tensorflow:loss = 10.921156, step = 16601 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.38\n",
      "INFO:tensorflow:loss = 13.191617, step = 16701 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.672\n",
      "INFO:tensorflow:loss = 13.251877, step = 16801 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.071\n",
      "INFO:tensorflow:loss = 10.329614, step = 16901 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.147\n",
      "INFO:tensorflow:loss = 21.459923, step = 17001 (0.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.272\n",
      "INFO:tensorflow:loss = 17.722027, step = 17101 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.215\n",
      "INFO:tensorflow:loss = 11.663305, step = 17201 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.587\n",
      "INFO:tensorflow:loss = 11.193661, step = 17301 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.538\n",
      "INFO:tensorflow:loss = 10.410952, step = 17401 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.067\n",
      "INFO:tensorflow:loss = 9.016793, step = 17501 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.308\n",
      "INFO:tensorflow:loss = 14.413925, step = 17601 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.683\n",
      "INFO:tensorflow:loss = 14.9679, step = 17701 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.356\n",
      "INFO:tensorflow:loss = 6.920013, step = 17801 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.372\n",
      "INFO:tensorflow:loss = 7.5668573, step = 17901 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.439\n",
      "INFO:tensorflow:loss = 10.91305, step = 18001 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.01\n",
      "INFO:tensorflow:loss = 15.786274, step = 18101 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.084\n",
      "INFO:tensorflow:loss = 16.592045, step = 18201 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.039\n",
      "INFO:tensorflow:loss = 6.6653023, step = 18301 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.595\n",
      "INFO:tensorflow:loss = 9.04872, step = 18401 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.187\n",
      "INFO:tensorflow:loss = 10.609932, step = 18501 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.061\n",
      "INFO:tensorflow:loss = 15.860295, step = 18601 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.528\n",
      "INFO:tensorflow:loss = 16.025322, step = 18701 (0.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.296\n",
      "INFO:tensorflow:loss = 6.553509, step = 18801 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.701\n",
      "INFO:tensorflow:loss = 11.098199, step = 18901 (0.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.833\n",
      "INFO:tensorflow:loss = 7.7619877, step = 19001 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.907\n",
      "INFO:tensorflow:loss = 8.804491, step = 19101 (0.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.122\n",
      "INFO:tensorflow:loss = 8.952744, step = 19201 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.509\n",
      "INFO:tensorflow:loss = 9.406612, step = 19301 (0.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.659\n",
      "INFO:tensorflow:loss = 8.9747925, step = 19401 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.726\n",
      "INFO:tensorflow:loss = 24.8541, step = 19501 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.293\n",
      "INFO:tensorflow:loss = 11.285887, step = 19601 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.395\n",
      "INFO:tensorflow:loss = 14.089956, step = 19701 (0.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.927\n",
      "INFO:tensorflow:loss = 15.157405, step = 19801 (0.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.157\n",
      "INFO:tensorflow:loss = 8.549145, step = 19901 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.131\n",
      "INFO:tensorflow:loss = 11.475817, step = 20001 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.957\n",
      "INFO:tensorflow:loss = 15.106624, step = 20101 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.991\n",
      "INFO:tensorflow:loss = 11.78604, step = 20201 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.933\n",
      "INFO:tensorflow:loss = 8.433885, step = 20301 (0.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.186\n",
      "INFO:tensorflow:loss = 10.178406, step = 20401 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.78\n",
      "INFO:tensorflow:loss = 6.687347, step = 20501 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.206\n",
      "INFO:tensorflow:loss = 17.720043, step = 20601 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.063\n",
      "INFO:tensorflow:loss = 7.0642977, step = 20701 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.889\n",
      "INFO:tensorflow:loss = 6.3213663, step = 20801 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.815\n",
      "INFO:tensorflow:loss = 9.490004, step = 20901 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.894\n",
      "INFO:tensorflow:loss = 10.621384, step = 21001 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.604\n",
      "INFO:tensorflow:loss = 6.9979258, step = 21101 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.952\n",
      "INFO:tensorflow:loss = 7.557773, step = 21201 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.14\n",
      "INFO:tensorflow:loss = 7.066731, step = 21301 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.716\n",
      "INFO:tensorflow:loss = 9.543501, step = 21401 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.794\n",
      "INFO:tensorflow:loss = 6.8694997, step = 21501 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.301\n",
      "INFO:tensorflow:loss = 11.131235, step = 21601 (0.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.275\n",
      "INFO:tensorflow:loss = 15.457277, step = 21701 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.571\n",
      "INFO:tensorflow:loss = 8.41108, step = 21801 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.221\n",
      "INFO:tensorflow:loss = 9.950062, step = 21901 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.375\n",
      "INFO:tensorflow:loss = 11.038912, step = 22001 (0.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.173\n",
      "INFO:tensorflow:loss = 7.696107, step = 22101 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.196\n",
      "INFO:tensorflow:loss = 7.644592, step = 22201 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 14.818699, step = 22301 (0.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.051\n",
      "INFO:tensorflow:loss = 8.23332, step = 22401 (0.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.275\n",
      "INFO:tensorflow:loss = 8.328533, step = 22501 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.018\n",
      "INFO:tensorflow:loss = 17.358458, step = 22601 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.995\n",
      "INFO:tensorflow:loss = 8.873623, step = 22701 (0.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.197\n",
      "INFO:tensorflow:loss = 11.540287, step = 22801 (0.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.668\n",
      "INFO:tensorflow:loss = 5.8864527, step = 22901 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.927\n",
      "INFO:tensorflow:loss = 12.036121, step = 23001 (0.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.733\n",
      "INFO:tensorflow:loss = 19.294659, step = 23101 (0.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.702\n",
      "INFO:tensorflow:loss = 7.576456, step = 23201 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.556\n",
      "INFO:tensorflow:loss = 11.085102, step = 23301 (0.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.703\n",
      "INFO:tensorflow:loss = 11.235025, step = 23401 (0.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.977\n",
      "INFO:tensorflow:loss = 9.448287, step = 23501 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.52\n",
      "INFO:tensorflow:loss = 9.365681, step = 23601 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.209\n",
      "INFO:tensorflow:loss = 15.509211, step = 23701 (0.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.225\n",
      "INFO:tensorflow:loss = 10.177959, step = 23801 (0.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.048\n",
      "INFO:tensorflow:loss = 12.884693, step = 23901 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.889\n",
      "INFO:tensorflow:loss = 10.371273, step = 24001 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.152\n",
      "INFO:tensorflow:loss = 11.042409, step = 24101 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.521\n",
      "INFO:tensorflow:loss = 9.402737, step = 24201 (0.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.965\n",
      "INFO:tensorflow:loss = 10.690136, step = 24301 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.177\n",
      "INFO:tensorflow:loss = 11.220217, step = 24401 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.358\n",
      "INFO:tensorflow:loss = 8.290361, step = 24501 (0.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.763\n",
      "INFO:tensorflow:loss = 8.966058, step = 24601 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.791\n",
      "INFO:tensorflow:loss = 7.18444, step = 24701 (0.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.251\n",
      "INFO:tensorflow:loss = 6.0371943, step = 24801 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.293\n",
      "INFO:tensorflow:loss = 9.525795, step = 24901 (0.544 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpbertm9f2/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 10.352339.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-31-07:27:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbertm9f2/bow_sparse/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-31-07:27:06\n",
      "INFO:tensorflow:Saving dict for global step 25000: accuracy = 0.83216, accuracy_baseline = 0.5, auc = 0.9037691, auc_precision_recall = 0.9056747, average_loss = 0.62630063, global_step = 25000, label/mean = 0.5, loss = 62.630062, precision = 0.83619434, prediction/mean = 0.49280727, recall = 0.82616\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25000: /tmp/tmpbertm9f2/bow_sparse/model.ckpt-25000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbertm9f2/bow_sparse/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpbertm9f2/bow_embeddings', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbd5d416a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpbertm9f2/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.290115, step = 1\n",
      "INFO:tensorflow:global_step/sec: 53.9788\n",
      "INFO:tensorflow:loss = 64.06616, step = 101 (1.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0112\n",
      "INFO:tensorflow:loss = 42.463287, step = 201 (1.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5542\n",
      "INFO:tensorflow:loss = 32.892033, step = 301 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6251\n",
      "INFO:tensorflow:loss = 26.01096, step = 401 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.257\n",
      "INFO:tensorflow:loss = 23.730406, step = 501 (1.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.2127\n",
      "INFO:tensorflow:loss = 34.41756, step = 601 (1.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.6292\n",
      "INFO:tensorflow:loss = 30.393295, step = 701 (1.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.2543\n",
      "INFO:tensorflow:loss = 19.073036, step = 801 (1.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.947\n",
      "INFO:tensorflow:loss = 28.07812, step = 901 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.0802\n",
      "INFO:tensorflow:loss = 24.48323, step = 1001 (1.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.3972\n",
      "INFO:tensorflow:loss = 22.271584, step = 1101 (1.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.5474\n",
      "INFO:tensorflow:loss = 17.508057, step = 1201 (1.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6369\n",
      "INFO:tensorflow:loss = 20.511295, step = 1301 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.357\n",
      "INFO:tensorflow:loss = 24.368275, step = 1401 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3854\n",
      "INFO:tensorflow:loss = 24.415878, step = 1501 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0455\n",
      "INFO:tensorflow:loss = 21.609314, step = 1601 (1.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6066\n",
      "INFO:tensorflow:loss = 20.311876, step = 1701 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.1217\n",
      "INFO:tensorflow:loss = 19.054977, step = 1801 (1.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4705\n",
      "INFO:tensorflow:loss = 14.929529, step = 1901 (1.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.2308\n",
      "INFO:tensorflow:loss = 12.248825, step = 2001 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.398\n",
      "INFO:tensorflow:loss = 21.472095, step = 2101 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.6071\n",
      "INFO:tensorflow:loss = 17.990393, step = 2201 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.1226\n",
      "INFO:tensorflow:loss = 17.172752, step = 2301 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.263\n",
      "INFO:tensorflow:loss = 29.357819, step = 2401 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.364\n",
      "INFO:tensorflow:loss = 13.511652, step = 2501 (0.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.479\n",
      "INFO:tensorflow:loss = 16.660324, step = 2601 (0.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9464\n",
      "INFO:tensorflow:loss = 14.228546, step = 2701 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.6992\n",
      "INFO:tensorflow:loss = 20.920395, step = 2801 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.574\n",
      "INFO:tensorflow:loss = 20.249493, step = 2901 (0.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.0741\n",
      "INFO:tensorflow:loss = 19.547283, step = 3001 (1.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.071\n",
      "INFO:tensorflow:loss = 11.092925, step = 3101 (0.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.603\n",
      "INFO:tensorflow:loss = 20.924757, step = 3201 (0.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.479\n",
      "INFO:tensorflow:loss = 7.889097, step = 3301 (0.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.928\n",
      "INFO:tensorflow:loss = 14.032579, step = 3401 (0.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.103\n",
      "INFO:tensorflow:loss = 19.816078, step = 3501 (0.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.315\n",
      "INFO:tensorflow:loss = 17.317297, step = 3601 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.031\n",
      "INFO:tensorflow:loss = 22.518684, step = 3701 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.669\n",
      "INFO:tensorflow:loss = 11.386877, step = 3801 (0.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.307\n",
      "INFO:tensorflow:loss = 15.756384, step = 3901 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.1833\n",
      "INFO:tensorflow:loss = 13.087809, step = 4001 (1.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.9889\n",
      "INFO:tensorflow:loss = 17.137362, step = 4101 (1.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5534\n",
      "INFO:tensorflow:loss = 17.037825, step = 4201 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.288\n",
      "INFO:tensorflow:loss = 8.821724, step = 4301 (0.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.5131\n",
      "INFO:tensorflow:loss = 12.154192, step = 4401 (1.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.0788\n",
      "INFO:tensorflow:loss = 11.009737, step = 4501 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.826\n",
      "INFO:tensorflow:loss = 13.972137, step = 4601 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.693\n",
      "INFO:tensorflow:loss = 16.528921, step = 4701 (0.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.9425\n",
      "INFO:tensorflow:loss = 9.908796, step = 4801 (1.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.458\n",
      "INFO:tensorflow:loss = 17.544827, step = 4901 (0.931 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.1972\n",
      "INFO:tensorflow:loss = 12.57632, step = 5001 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.7795\n",
      "INFO:tensorflow:loss = 9.418466, step = 5101 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.8929\n",
      "INFO:tensorflow:loss = 15.568047, step = 5201 (1.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.079\n",
      "INFO:tensorflow:loss = 10.743397, step = 5301 (1.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.96\n",
      "INFO:tensorflow:loss = 20.466436, step = 5401 (0.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.4223\n",
      "INFO:tensorflow:loss = 6.120171, step = 5501 (1.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.977\n",
      "INFO:tensorflow:loss = 8.784832, step = 5601 (0.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.355\n",
      "INFO:tensorflow:loss = 16.833794, step = 5701 (1.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.629\n",
      "INFO:tensorflow:loss = 17.294003, step = 5801 (1.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.962\n",
      "INFO:tensorflow:loss = 6.184518, step = 5901 (0.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.3409\n",
      "INFO:tensorflow:loss = 8.051017, step = 6001 (1.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.787\n",
      "INFO:tensorflow:loss = 14.832475, step = 6101 (0.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.454\n",
      "INFO:tensorflow:loss = 11.864231, step = 6201 (0.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1466\n",
      "INFO:tensorflow:loss = 8.1883335, step = 6301 (1.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.352\n",
      "INFO:tensorflow:loss = 17.32628, step = 6401 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.5548\n",
      "INFO:tensorflow:loss = 9.038116, step = 6501 (1.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.7773\n",
      "INFO:tensorflow:loss = 9.5122595, step = 6601 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.3495\n",
      "INFO:tensorflow:loss = 4.157303, step = 6701 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1769\n",
      "INFO:tensorflow:loss = 4.2079043, step = 6801 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.479\n",
      "INFO:tensorflow:loss = 9.792944, step = 6901 (0.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.218\n",
      "INFO:tensorflow:loss = 7.067503, step = 7001 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.081\n",
      "INFO:tensorflow:loss = 3.5442238, step = 7101 (0.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.236\n",
      "INFO:tensorflow:loss = 9.902483, step = 7201 (0.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.18591, step = 7301 (1.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.582\n",
      "INFO:tensorflow:loss = 7.2644916, step = 7401 (0.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9659\n",
      "INFO:tensorflow:loss = 5.1328626, step = 7501 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.2416\n",
      "INFO:tensorflow:loss = 5.2140512, step = 7601 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.6076\n",
      "INFO:tensorflow:loss = 6.068268, step = 7701 (1.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7928\n",
      "INFO:tensorflow:loss = 8.8741865, step = 7801 (1.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5129\n",
      "INFO:tensorflow:loss = 8.336194, step = 7901 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.1013\n",
      "INFO:tensorflow:loss = 4.0754237, step = 8001 (1.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.0683\n",
      "INFO:tensorflow:loss = 3.8319693, step = 8101 (1.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.1296\n",
      "INFO:tensorflow:loss = 5.104166, step = 8201 (1.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6538\n",
      "INFO:tensorflow:loss = 11.057835, step = 8301 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.5993\n",
      "INFO:tensorflow:loss = 5.0772853, step = 8401 (1.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7816\n",
      "INFO:tensorflow:loss = 6.0357213, step = 8501 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.787\n",
      "INFO:tensorflow:loss = 6.29481, step = 8601 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.0893\n",
      "INFO:tensorflow:loss = 10.991799, step = 8701 (1.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.3581\n",
      "INFO:tensorflow:loss = 3.7851305, step = 8801 (1.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.191\n",
      "INFO:tensorflow:loss = 4.321031, step = 8901 (0.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7858\n",
      "INFO:tensorflow:loss = 1.9926183, step = 9001 (1.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.336\n",
      "INFO:tensorflow:loss = 3.8914795, step = 9101 (0.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.721\n",
      "INFO:tensorflow:loss = 5.3365874, step = 9201 (0.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.5195\n",
      "INFO:tensorflow:loss = 4.0872526, step = 9301 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.6735\n",
      "INFO:tensorflow:loss = 12.858541, step = 9401 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.6079\n",
      "INFO:tensorflow:loss = 1.7744688, step = 9501 (1.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0118\n",
      "INFO:tensorflow:loss = 3.1394682, step = 9601 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.063\n",
      "INFO:tensorflow:loss = 3.6711671, step = 9701 (0.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.398\n",
      "INFO:tensorflow:loss = 5.3095207, step = 9801 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2375\n",
      "INFO:tensorflow:loss = 7.7297544, step = 9901 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.8858\n",
      "INFO:tensorflow:loss = 13.883955, step = 10001 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.343\n",
      "INFO:tensorflow:loss = 6.4298754, step = 10101 (1.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.1693\n",
      "INFO:tensorflow:loss = 5.5061703, step = 10201 (1.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.1133\n",
      "INFO:tensorflow:loss = 4.04543, step = 10301 (1.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.669\n",
      "INFO:tensorflow:loss = 7.3670073, step = 10401 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.1249\n",
      "INFO:tensorflow:loss = 2.1409671, step = 10501 (1.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.295\n",
      "INFO:tensorflow:loss = 2.4950356, step = 10601 (0.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.823\n",
      "INFO:tensorflow:loss = 3.4876807, step = 10701 (0.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1702\n",
      "INFO:tensorflow:loss = 37.975113, step = 10801 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.741\n",
      "INFO:tensorflow:loss = 2.1320152, step = 10901 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6129\n",
      "INFO:tensorflow:loss = 4.4750414, step = 11001 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.931\n",
      "INFO:tensorflow:loss = 2.9196262, step = 11101 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.8802\n",
      "INFO:tensorflow:loss = 2.12527, step = 11201 (1.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0601\n",
      "INFO:tensorflow:loss = 1.9952781, step = 11301 (1.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.005\n",
      "INFO:tensorflow:loss = 2.9671457, step = 11401 (0.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.6016\n",
      "INFO:tensorflow:loss = 1.3290176, step = 11501 (1.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.709\n",
      "INFO:tensorflow:loss = 2.2555408, step = 11601 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.448\n",
      "INFO:tensorflow:loss = 2.740346, step = 11701 (0.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.4038\n",
      "INFO:tensorflow:loss = 6.680732, step = 11801 (1.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.589\n",
      "INFO:tensorflow:loss = 2.307526, step = 11901 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.9851\n",
      "INFO:tensorflow:loss = 4.569851, step = 12001 (1.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.8888\n",
      "INFO:tensorflow:loss = 1.3856151, step = 12101 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.805\n",
      "INFO:tensorflow:loss = 4.1353745, step = 12201 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.1384\n",
      "INFO:tensorflow:loss = 2.1788435, step = 12301 (1.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.892\n",
      "INFO:tensorflow:loss = 4.3011265, step = 12401 (0.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.2362\n",
      "INFO:tensorflow:loss = 2.0526514, step = 12501 (1.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.807\n",
      "INFO:tensorflow:loss = 2.17343, step = 12601 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.2439\n",
      "INFO:tensorflow:loss = 1.7674088, step = 12701 (1.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.9641\n",
      "INFO:tensorflow:loss = 2.2896824, step = 12801 (1.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.817\n",
      "INFO:tensorflow:loss = 1.5096205, step = 12901 (0.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.2906\n",
      "INFO:tensorflow:loss = 1.4505312, step = 13001 (1.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.322\n",
      "INFO:tensorflow:loss = 2.3529277, step = 13101 (0.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.536\n",
      "INFO:tensorflow:loss = 1.6446886, step = 13201 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.5935\n",
      "INFO:tensorflow:loss = 1.4852138, step = 13301 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.914\n",
      "INFO:tensorflow:loss = 1.7111866, step = 13401 (0.983 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.126\n",
      "INFO:tensorflow:loss = 1.0916641, step = 13501 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6752\n",
      "INFO:tensorflow:loss = 2.088618, step = 13601 (1.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.792\n",
      "INFO:tensorflow:loss = 2.4286594, step = 13701 (1.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.453\n",
      "INFO:tensorflow:loss = 36.02293, step = 13801 (1.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.2217\n",
      "INFO:tensorflow:loss = 3.9437914, step = 13901 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.1791\n",
      "INFO:tensorflow:loss = 1.9170372, step = 14001 (1.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5514\n",
      "INFO:tensorflow:loss = 3.4002824, step = 14101 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.4322\n",
      "INFO:tensorflow:loss = 1.5258092, step = 14201 (1.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.7445\n",
      "INFO:tensorflow:loss = 0.9892807, step = 14301 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.901\n",
      "INFO:tensorflow:loss = 0.68049884, step = 14401 (0.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5581\n",
      "INFO:tensorflow:loss = 1.3272533, step = 14501 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.739\n",
      "INFO:tensorflow:loss = 1.1109297, step = 14601 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.135\n",
      "INFO:tensorflow:loss = 0.89817786, step = 14701 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.0733\n",
      "INFO:tensorflow:loss = 0.5503086, step = 14801 (1.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.884\n",
      "INFO:tensorflow:loss = 0.9924723, step = 14901 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.5748\n",
      "INFO:tensorflow:loss = 1.0504041, step = 15001 (1.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.732\n",
      "INFO:tensorflow:loss = 1.3003992, step = 15101 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.675\n",
      "INFO:tensorflow:loss = 0.41740197, step = 15201 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7999\n",
      "INFO:tensorflow:loss = 0.664302, step = 15301 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7156\n",
      "INFO:tensorflow:loss = 1.310612, step = 15401 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3048\n",
      "INFO:tensorflow:loss = 0.92998147, step = 15501 (1.145 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 97.1091\n",
      "INFO:tensorflow:loss = 1.0907006, step = 15601 (1.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5431\n",
      "INFO:tensorflow:loss = 2.184969, step = 15701 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.9008\n",
      "INFO:tensorflow:loss = 0.99185234, step = 15801 (1.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.7726\n",
      "INFO:tensorflow:loss = 1.3348837, step = 15901 (1.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.2277\n",
      "INFO:tensorflow:loss = 1.104886, step = 16001 (1.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1886\n",
      "INFO:tensorflow:loss = 0.78080106, step = 16101 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6431\n",
      "INFO:tensorflow:loss = 8.895581, step = 16201 (1.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.9712\n",
      "INFO:tensorflow:loss = 0.5097958, step = 16301 (1.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.911\n",
      "INFO:tensorflow:loss = 0.995707, step = 16401 (1.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.861\n",
      "INFO:tensorflow:loss = 0.81793267, step = 16501 (1.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.3275\n",
      "INFO:tensorflow:loss = 0.9454682, step = 16601 (1.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.168\n",
      "INFO:tensorflow:loss = 0.5807042, step = 16701 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.662\n",
      "INFO:tensorflow:loss = 0.8219875, step = 16801 (1.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.091\n",
      "INFO:tensorflow:loss = 0.7831218, step = 16901 (0.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.1212\n",
      "INFO:tensorflow:loss = 0.63195646, step = 17001 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.378\n",
      "INFO:tensorflow:loss = 0.7500844, step = 17101 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9058\n",
      "INFO:tensorflow:loss = 0.7139896, step = 17201 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.77\n",
      "INFO:tensorflow:loss = 0.5134099, step = 17301 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.096\n",
      "INFO:tensorflow:loss = 0.9003853, step = 17401 (0.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.406\n",
      "INFO:tensorflow:loss = 0.67834806, step = 17501 (1.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.6898\n",
      "INFO:tensorflow:loss = 0.36839354, step = 17601 (1.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.091\n",
      "INFO:tensorflow:loss = 0.5230962, step = 17701 (0.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.109\n",
      "INFO:tensorflow:loss = 0.5886276, step = 17801 (0.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.878\n",
      "INFO:tensorflow:loss = 0.90103096, step = 17901 (0.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.631\n",
      "INFO:tensorflow:loss = 0.72660804, step = 18001 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.564\n",
      "INFO:tensorflow:loss = 0.7147901, step = 18101 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.745\n",
      "INFO:tensorflow:loss = 0.35151857, step = 18201 (0.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.1365\n",
      "INFO:tensorflow:loss = 0.89801794, step = 18301 (1.041 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.162\n",
      "INFO:tensorflow:loss = 1.3113332, step = 18401 (0.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.6348\n",
      "INFO:tensorflow:loss = 0.39651507, step = 18501 (1.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.223\n",
      "INFO:tensorflow:loss = 0.43688393, step = 18601 (0.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1886\n",
      "INFO:tensorflow:loss = 0.25169885, step = 18701 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.8982\n",
      "INFO:tensorflow:loss = 0.509441, step = 18801 (1.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.909\n",
      "INFO:tensorflow:loss = 0.33092254, step = 18901 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.4381\n",
      "INFO:tensorflow:loss = 0.26579386, step = 19001 (1.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.068\n",
      "INFO:tensorflow:loss = 0.48952675, step = 19101 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.985\n",
      "INFO:tensorflow:loss = 0.71630895, step = 19201 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.8781\n",
      "INFO:tensorflow:loss = 0.4262475, step = 19301 (1.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.6648\n",
      "INFO:tensorflow:loss = 0.81859034, step = 19401 (1.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4091\n",
      "INFO:tensorflow:loss = 0.40827045, step = 19501 (1.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.0748\n",
      "INFO:tensorflow:loss = 0.40532267, step = 19601 (1.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7186\n",
      "INFO:tensorflow:loss = 0.428684, step = 19701 (1.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.4283\n",
      "INFO:tensorflow:loss = 0.5474764, step = 19801 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.967\n",
      "INFO:tensorflow:loss = 0.44864303, step = 19901 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.1961\n",
      "INFO:tensorflow:loss = 0.27181306, step = 20001 (1.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.535\n",
      "INFO:tensorflow:loss = 0.7622713, step = 20101 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.7539\n",
      "INFO:tensorflow:loss = 0.46157056, step = 20201 (1.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.672\n",
      "INFO:tensorflow:loss = 0.35504666, step = 20301 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.228\n",
      "INFO:tensorflow:loss = 0.36286277, step = 20401 (0.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.2059\n",
      "INFO:tensorflow:loss = 0.5274589, step = 20501 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.937\n",
      "INFO:tensorflow:loss = 0.47634202, step = 20601 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.359\n",
      "INFO:tensorflow:loss = 0.5032183, step = 20701 (0.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.901\n",
      "INFO:tensorflow:loss = 0.545918, step = 20801 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.906\n",
      "INFO:tensorflow:loss = 0.33273643, step = 20901 (0.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.9652\n",
      "INFO:tensorflow:loss = 0.5257385, step = 21001 (1.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.469\n",
      "INFO:tensorflow:loss = 0.47060737, step = 21101 (0.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.412\n",
      "INFO:tensorflow:loss = 0.687944, step = 21201 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.6138\n",
      "INFO:tensorflow:loss = 0.7677691, step = 21301 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.35\n",
      "INFO:tensorflow:loss = 0.25116438, step = 21401 (0.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.083\n",
      "INFO:tensorflow:loss = 0.5087242, step = 21501 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.492\n",
      "INFO:tensorflow:loss = 0.34831136, step = 21601 (0.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7591\n",
      "INFO:tensorflow:loss = 0.3545758, step = 21701 (1.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.701\n",
      "INFO:tensorflow:loss = 0.27659464, step = 21801 (0.983 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.203\n",
      "INFO:tensorflow:loss = 0.2692232, step = 21901 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.9252\n",
      "INFO:tensorflow:loss = 0.34848654, step = 22001 (1.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.775\n",
      "INFO:tensorflow:loss = 0.31679827, step = 22101 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.8879\n",
      "INFO:tensorflow:loss = 0.34828925, step = 22201 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.371\n",
      "INFO:tensorflow:loss = 0.5419936, step = 22301 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.1035\n",
      "INFO:tensorflow:loss = 0.20606868, step = 22401 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.4945\n",
      "INFO:tensorflow:loss = 0.31146094, step = 22501 (1.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.243\n",
      "INFO:tensorflow:loss = 0.34317842, step = 22601 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.313\n",
      "INFO:tensorflow:loss = 0.28080025, step = 22701 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.863\n",
      "INFO:tensorflow:loss = 0.30192006, step = 22801 (0.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.336\n",
      "INFO:tensorflow:loss = 0.19384515, step = 22901 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0828\n",
      "INFO:tensorflow:loss = 0.13934152, step = 23001 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.145\n",
      "INFO:tensorflow:loss = 0.41979337, step = 23101 (0.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.93\n",
      "INFO:tensorflow:loss = 0.35972428, step = 23201 (0.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.6501\n",
      "INFO:tensorflow:loss = 0.19009206, step = 23301 (1.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.494\n",
      "INFO:tensorflow:loss = 0.2424433, step = 23401 (0.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.8263\n",
      "INFO:tensorflow:loss = 0.28681356, step = 23501 (1.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.983\n",
      "INFO:tensorflow:loss = 0.13873349, step = 23601 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.639\n",
      "INFO:tensorflow:loss = 0.35582262, step = 23701 (0.956 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 100.028\n",
      "INFO:tensorflow:loss = 0.7369575, step = 23801 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.2187\n",
      "INFO:tensorflow:loss = 0.36832663, step = 23901 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.1603\n",
      "INFO:tensorflow:loss = 0.26847604, step = 24001 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.4665\n",
      "INFO:tensorflow:loss = 0.33083647, step = 24101 (1.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.927\n",
      "INFO:tensorflow:loss = 0.45773053, step = 24201 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.6596\n",
      "INFO:tensorflow:loss = 0.21454957, step = 24301 (1.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.5436\n",
      "INFO:tensorflow:loss = 0.14346896, step = 24401 (1.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.5828\n",
      "INFO:tensorflow:loss = 0.20345464, step = 24501 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.934\n",
      "INFO:tensorflow:loss = 0.3169082, step = 24601 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.6805\n",
      "INFO:tensorflow:loss = 0.3038068, step = 24701 (1.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.2808\n",
      "INFO:tensorflow:loss = 0.23007184, step = 24801 (1.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.941\n",
      "INFO:tensorflow:loss = 0.2985672, step = 24901 (0.981 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25000 into /tmp/tmpbertm9f2/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22441447.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-31-07:32:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbertm9f2/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-31-07:32:26\n",
      "INFO:tensorflow:Saving dict for global step 25000: accuracy = 0.8324, accuracy_baseline = 0.5, auc = 0.8711476, auc_precision_recall = 0.8921898, average_loss = 1.5446042, global_step = 25000, label/mean = 0.5, loss = 154.46042, precision = 0.8395718, prediction/mean = 0.49052253, recall = 0.82184\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25000: /tmp/tmpbertm9f2/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbertm9f2/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpbertm9f2/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbd2d85f780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpbertm9f2/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.70442736, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.1563\n",
      "INFO:tensorflow:loss = 0.6945028, step = 101 (9.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1822\n",
      "INFO:tensorflow:loss = 0.6452922, step = 201 (8.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1442\n",
      "INFO:tensorflow:loss = 0.546823, step = 301 (8.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1394\n",
      "INFO:tensorflow:loss = 0.55680555, step = 401 (8.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1232\n",
      "INFO:tensorflow:loss = 0.45933262, step = 501 (8.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2332\n",
      "INFO:tensorflow:loss = 0.4092895, step = 601 (8.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1816\n",
      "INFO:tensorflow:loss = 0.42056957, step = 701 (8.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1755\n",
      "INFO:tensorflow:loss = 0.35348922, step = 801 (8.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1205\n",
      "INFO:tensorflow:loss = 0.41100806, step = 901 (8.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4791\n",
      "INFO:tensorflow:loss = 0.38668033, step = 1001 (9.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0885\n",
      "INFO:tensorflow:loss = 0.3480369, step = 1101 (9.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9116\n",
      "INFO:tensorflow:loss = 0.266026, step = 1201 (9.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9665\n",
      "INFO:tensorflow:loss = 0.3314615, step = 1301 (9.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.152\n",
      "INFO:tensorflow:loss = 0.27548838, step = 1401 (8.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0011\n",
      "INFO:tensorflow:loss = 0.23185787, step = 1501 (9.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1805\n",
      "INFO:tensorflow:loss = 0.30618373, step = 1601 (8.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1926\n",
      "INFO:tensorflow:loss = 0.35295898, step = 1701 (8.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1746\n",
      "INFO:tensorflow:loss = 0.27155063, step = 1801 (8.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2312\n",
      "INFO:tensorflow:loss = 0.24388261, step = 1901 (8.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1385\n",
      "INFO:tensorflow:loss = 0.32378137, step = 2001 (8.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2275\n",
      "INFO:tensorflow:loss = 0.28870663, step = 2101 (8.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9972\n",
      "INFO:tensorflow:loss = 0.19185337, step = 2201 (9.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7592\n",
      "INFO:tensorflow:loss = 0.24488914, step = 2301 (9.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0268\n",
      "INFO:tensorflow:loss = 0.17518213, step = 2401 (9.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7698\n",
      "INFO:tensorflow:loss = 0.19607943, step = 2501 (9.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1588\n",
      "INFO:tensorflow:loss = 0.30312848, step = 2601 (8.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1608\n",
      "INFO:tensorflow:loss = 0.21019219, step = 2701 (8.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1748\n",
      "INFO:tensorflow:loss = 0.19478485, step = 2801 (8.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2622\n",
      "INFO:tensorflow:loss = 0.12451965, step = 2901 (8.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1055\n",
      "INFO:tensorflow:loss = 0.21167904, step = 3001 (9.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1778\n",
      "INFO:tensorflow:loss = 0.2530627, step = 3101 (8.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2172\n",
      "INFO:tensorflow:loss = 0.12464066, step = 3201 (8.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5724\n",
      "INFO:tensorflow:loss = 0.103118226, step = 3301 (9.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4514\n",
      "INFO:tensorflow:loss = 0.18723172, step = 3401 (9.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1097\n",
      "INFO:tensorflow:loss = 0.2279052, step = 3501 (9.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1998\n",
      "INFO:tensorflow:loss = 0.18827859, step = 3601 (8.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2056\n",
      "INFO:tensorflow:loss = 0.20986502, step = 3701 (8.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1029\n",
      "INFO:tensorflow:loss = 0.12945282, step = 3801 (9.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2404\n",
      "INFO:tensorflow:loss = 0.0972945, step = 3901 (8.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1283\n",
      "INFO:tensorflow:loss = 0.13499767, step = 4001 (8.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2606\n",
      "INFO:tensorflow:loss = 0.14639701, step = 4101 (8.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1588\n",
      "INFO:tensorflow:loss = 0.09378418, step = 4201 (8.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9435\n",
      "INFO:tensorflow:loss = 0.14755327, step = 4301 (9.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1152\n",
      "INFO:tensorflow:loss = 0.16238274, step = 4401 (8.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1298\n",
      "INFO:tensorflow:loss = 0.16833198, step = 4501 (8.985 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 11.1967\n",
      "INFO:tensorflow:loss = 0.11686996, step = 4601 (8.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2038\n",
      "INFO:tensorflow:loss = 0.20006679, step = 4701 (8.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1419\n",
      "INFO:tensorflow:loss = 0.072332, step = 4801 (8.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1901\n",
      "INFO:tensorflow:loss = 0.16852795, step = 4901 (8.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0188\n",
      "INFO:tensorflow:loss = 0.08740861, step = 5001 (9.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2378\n",
      "INFO:tensorflow:loss = 0.16309862, step = 5101 (8.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1957\n",
      "INFO:tensorflow:loss = 0.06693948, step = 5201 (8.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.204\n",
      "INFO:tensorflow:loss = 0.21703503, step = 5301 (8.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.195\n",
      "INFO:tensorflow:loss = 0.16363353, step = 5401 (8.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1339\n",
      "INFO:tensorflow:loss = 0.077605635, step = 5501 (8.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2207\n",
      "INFO:tensorflow:loss = 0.1594604, step = 5601 (8.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1401\n",
      "INFO:tensorflow:loss = 0.06672528, step = 5701 (8.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1158\n",
      "INFO:tensorflow:loss = 0.094511546, step = 5801 (8.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1925\n",
      "INFO:tensorflow:loss = 0.10413254, step = 5901 (8.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1336\n",
      "INFO:tensorflow:loss = 0.12483286, step = 6001 (8.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2203\n",
      "INFO:tensorflow:loss = 0.0404743, step = 6101 (8.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0957\n",
      "INFO:tensorflow:loss = 0.16559131, step = 6201 (9.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.651\n",
      "INFO:tensorflow:loss = 0.08888824, step = 6301 (9.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1692\n",
      "INFO:tensorflow:loss = 0.089768946, step = 6401 (8.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.121\n",
      "INFO:tensorflow:loss = 0.14838177, step = 6501 (8.992 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6568 into /tmp/tmpbertm9f2/cnn/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.932\n",
      "INFO:tensorflow:loss = 0.05460541, step = 6601 (9.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1448\n",
      "INFO:tensorflow:loss = 0.0824773, step = 6701 (8.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1138\n",
      "INFO:tensorflow:loss = 0.18140712, step = 6801 (8.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8763\n",
      "INFO:tensorflow:loss = 0.103324644, step = 6901 (9.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5549\n",
      "INFO:tensorflow:loss = 0.13834205, step = 7001 (9.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2583\n",
      "INFO:tensorflow:loss = 0.09384686, step = 7101 (9.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.4761\n",
      "INFO:tensorflow:loss = 0.095987245, step = 7201 (10.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3262\n",
      "INFO:tensorflow:loss = 0.07297052, step = 7301 (9.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8211\n",
      "INFO:tensorflow:loss = 0.07859703, step = 7401 (9.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2458\n",
      "INFO:tensorflow:loss = 0.0784178, step = 7501 (9.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.31799\n",
      "INFO:tensorflow:loss = 0.08045335, step = 7601 (12.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.90194\n",
      "INFO:tensorflow:loss = 0.10610692, step = 7701 (12.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12179\n",
      "INFO:tensorflow:loss = 0.05961062, step = 7801 (11.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.6711\n",
      "INFO:tensorflow:loss = 0.03995086, step = 7901 (11.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.86956\n",
      "INFO:tensorflow:loss = 0.074795395, step = 8001 (10.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2306\n",
      "INFO:tensorflow:loss = 0.063795514, step = 8101 (9.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.70474\n",
      "INFO:tensorflow:loss = 0.10723617, step = 8201 (10.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4079\n",
      "INFO:tensorflow:loss = 0.019400476, step = 8301 (9.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2122\n",
      "INFO:tensorflow:loss = 0.082283884, step = 8401 (9.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6293\n",
      "INFO:tensorflow:loss = 0.07992529, step = 8501 (9.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9061\n",
      "INFO:tensorflow:loss = 0.053917374, step = 8601 (9.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5235\n",
      "INFO:tensorflow:loss = 0.05461361, step = 8701 (9.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4995\n",
      "INFO:tensorflow:loss = 0.10845321, step = 8801 (9.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2087\n",
      "INFO:tensorflow:loss = 0.04691511, step = 8901 (8.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1319\n",
      "INFO:tensorflow:loss = 0.07515666, step = 9001 (8.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2363\n",
      "INFO:tensorflow:loss = 0.029077461, step = 9101 (8.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0901\n",
      "INFO:tensorflow:loss = 0.057779036, step = 9201 (8.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.96208\n",
      "INFO:tensorflow:loss = 0.1219322, step = 9301 (10.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.48954\n",
      "INFO:tensorflow:loss = 0.038256716, step = 9401 (10.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.78631\n",
      "INFO:tensorflow:loss = 0.052114192, step = 9501 (10.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.28066\n",
      "INFO:tensorflow:loss = 0.02558365, step = 9601 (12.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.68381\n",
      "INFO:tensorflow:loss = 0.048310705, step = 9701 (11.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.35099\n",
      "INFO:tensorflow:loss = 0.027693, step = 9801 (11.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3187\n",
      "INFO:tensorflow:loss = 0.119449206, step = 9901 (9.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.77239\n",
      "INFO:tensorflow:loss = 0.014949038, step = 10001 (10.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.91427\n",
      "INFO:tensorflow:loss = 0.025726693, step = 10101 (12.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.24031\n",
      "INFO:tensorflow:loss = 0.100474946, step = 10201 (13.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.77565\n",
      "INFO:tensorflow:loss = 0.12188503, step = 10301 (12.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.10472\n",
      "INFO:tensorflow:loss = 0.06315241, step = 10401 (10.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3679\n",
      "INFO:tensorflow:loss = 0.05595532, step = 10501 (9.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7685\n",
      "INFO:tensorflow:loss = 0.033354074, step = 10601 (9.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6331\n",
      "INFO:tensorflow:loss = 0.02538699, step = 10701 (9.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.99025\n",
      "INFO:tensorflow:loss = 0.09706503, step = 10801 (10.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8402\n",
      "INFO:tensorflow:loss = 0.09394414, step = 10901 (9.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7991\n",
      "INFO:tensorflow:loss = 0.023984484, step = 11001 (9.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0978\n",
      "INFO:tensorflow:loss = 0.036867477, step = 11101 (9.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8688\n",
      "INFO:tensorflow:loss = 0.083382756, step = 11201 (9.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7869\n",
      "INFO:tensorflow:loss = 0.07131137, step = 11301 (9.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8504\n",
      "INFO:tensorflow:loss = 0.05284035, step = 11401 (9.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3922\n",
      "INFO:tensorflow:loss = 0.081002094, step = 11501 (9.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5123\n",
      "INFO:tensorflow:loss = 0.019802408, step = 11601 (9.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8779\n",
      "INFO:tensorflow:loss = 0.010005571, step = 11701 (9.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7709\n",
      "INFO:tensorflow:loss = 0.042661514, step = 11801 (9.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6283\n",
      "INFO:tensorflow:loss = 0.02654269, step = 11901 (9.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5883\n",
      "INFO:tensorflow:loss = 0.015836654, step = 12001 (9.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.712\n",
      "INFO:tensorflow:loss = 0.025074234, step = 12101 (9.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1914\n",
      "INFO:tensorflow:loss = 0.014354229, step = 12201 (9.812 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.96286\n",
      "INFO:tensorflow:loss = 0.10597233, step = 12301 (10.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.319\n",
      "INFO:tensorflow:loss = 0.006965357, step = 12401 (9.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6624\n",
      "INFO:tensorflow:loss = 0.008488873, step = 12501 (9.379 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12568 into /tmp/tmpbertm9f2/cnn/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 9.69664\n",
      "INFO:tensorflow:loss = 0.033302393, step = 12601 (10.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.778\n",
      "INFO:tensorflow:loss = 0.09231815, step = 12701 (9.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7608\n",
      "INFO:tensorflow:loss = 0.014280795, step = 12801 (9.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.662\n",
      "INFO:tensorflow:loss = 0.016829344, step = 12901 (9.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7951\n",
      "INFO:tensorflow:loss = 0.07692009, step = 13001 (9.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9688\n",
      "INFO:tensorflow:loss = 0.048795562, step = 13101 (9.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4973\n",
      "INFO:tensorflow:loss = 0.009324282, step = 13201 (9.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2583\n",
      "INFO:tensorflow:loss = 0.11780804, step = 13301 (9.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1382\n",
      "INFO:tensorflow:loss = 0.03559673, step = 13401 (8.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0667\n",
      "INFO:tensorflow:loss = 0.04000253, step = 13501 (9.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5626\n",
      "INFO:tensorflow:loss = 0.037045285, step = 13601 (9.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3477\n",
      "INFO:tensorflow:loss = 0.004364384, step = 13701 (9.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3322\n",
      "INFO:tensorflow:loss = 0.055416208, step = 13801 (9.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9235\n",
      "INFO:tensorflow:loss = 0.034560412, step = 13901 (9.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7554\n",
      "INFO:tensorflow:loss = 0.0074656163, step = 14001 (9.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8911\n",
      "INFO:tensorflow:loss = 0.025497802, step = 14101 (9.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6732\n",
      "INFO:tensorflow:loss = 0.04670893, step = 14201 (9.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2899\n",
      "INFO:tensorflow:loss = 0.012884859, step = 14301 (9.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0086\n",
      "INFO:tensorflow:loss = 0.013239651, step = 14401 (9.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2881\n",
      "INFO:tensorflow:loss = 0.022620851, step = 14501 (9.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0592\n",
      "INFO:tensorflow:loss = 0.078927286, step = 14601 (9.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3696\n",
      "INFO:tensorflow:loss = 0.029679751, step = 14701 (9.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7732\n",
      "INFO:tensorflow:loss = 0.02139488, step = 14801 (9.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9776\n",
      "INFO:tensorflow:loss = 0.012271147, step = 14901 (9.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8867\n",
      "INFO:tensorflow:loss = 0.026282106, step = 15001 (9.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7667\n",
      "INFO:tensorflow:loss = 0.012252008, step = 15101 (9.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7693\n",
      "INFO:tensorflow:loss = 0.0189827, step = 15201 (9.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4529\n",
      "INFO:tensorflow:loss = 0.05923726, step = 15301 (9.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.80287\n",
      "INFO:tensorflow:loss = 0.0059849606, step = 15401 (10.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.29295\n",
      "INFO:tensorflow:loss = 0.016561706, step = 15501 (10.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3623\n",
      "INFO:tensorflow:loss = 0.01436779, step = 15601 (9.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0548\n",
      "INFO:tensorflow:loss = 0.027853897, step = 15701 (9.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2877\n",
      "INFO:tensorflow:loss = 0.06429804, step = 15801 (9.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0982\n",
      "INFO:tensorflow:loss = 0.018932266, step = 15901 (9.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6752\n",
      "INFO:tensorflow:loss = 0.014409895, step = 16001 (9.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.70243\n",
      "INFO:tensorflow:loss = 0.0028779446, step = 16101 (10.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6839\n",
      "INFO:tensorflow:loss = 0.010154213, step = 16201 (9.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3902\n",
      "INFO:tensorflow:loss = 0.0041491264, step = 16301 (9.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7127\n",
      "INFO:tensorflow:loss = 0.012750894, step = 16401 (9.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7455\n",
      "INFO:tensorflow:loss = 0.021404456, step = 16501 (9.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0995\n",
      "INFO:tensorflow:loss = 0.0054863375, step = 16601 (9.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5044\n",
      "INFO:tensorflow:loss = 0.03985744, step = 16701 (9.522 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fcf6d68f937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m                                         params=params)\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \"\"\"### LSTM Networks\n",
      "\u001b[0;32m<ipython-input-1-7fcf6d68f937>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Save a reference to the classifier to run predictions later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mall_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1171\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1172\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1449\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    581\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1060\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "\n",
    "\"\"\"### Loading the data\n",
    "Keras provides a convenient handler for importing the dataset which is also available as a serialized numpy array `.npz` file to download [here]( https://s3.amazonaws.com/text-datasets/imdb.npz). Each review consists of a series of word indexes that go from $4$ (the most frequent word in the dataset, **the**) to $4999$, which corresponds to **orange**. Index $1$ represents the beginning of the sentence and the index $2$ is assigned to all unknown (also known as *out-of-vocabulary* or *OOV*) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency. We are not convering these techniques in this post, but you can take a look at [this chapter](http://www.nltk.org/book/ch03.html) of the NLTK book to learn more.\n",
    "It's standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. After we've loaded the data in memory we pad each of the sentences with $-1$ to a fixed size (here: $200$) so that we have two $2$-dimensional $25000\\times200$ arrays for training and testing respectively.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Should we not use keras and rewrite this logic?\n",
    "print(\"Loading data...\")\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(\n",
    "    num_words=vocab_size)\n",
    "print(len(y_train), \"train sequences\")\n",
    "print(len(y_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size, \n",
    "                                 padding='post', \n",
    "                                 value=0)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size, \n",
    "                                padding='post', \n",
    "                                value=0)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "\"\"\"We can use the word index map to inspect how the first review looks like.\"\"\"\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "word_inverted_index = {v: k for k, v in word_index.items()}\n",
    "# The first indexes in the map are reserved to represet things other than tokens\n",
    "index_offset = 3\n",
    "word_inverted_index[-1 - index_offset] = '_' # Padding at the end\n",
    "word_inverted_index[ 1 - index_offset] = '>' # Start of the sentence\n",
    "word_inverted_index[ 2 - index_offset] = '?' # OOV\n",
    "word_inverted_index[ 3 - index_offset] = ''  # Un-used\n",
    "\n",
    "def index_to_text(indexes):\n",
    "    return ' '.join([word_inverted_index[i - index_offset] for i in indexes])\n",
    "\n",
    "print(index_to_text(x_train_variable[0]))\n",
    "\n",
    "\"\"\"## Building Estimators\n",
    "In the next section we will build several models to make predictions for the labels in the dataset. We will first use canned estimators and then create custom ones for the task. We recommend that you check out [this blog post](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html) that explains how to use the `tf.feature_column` module to standardize and abstract how raw input data is processed and [the following one](https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html) that covers in depth how to work with Estimators.\n",
    "### From arrays to tensors\n",
    "There's one more thing we need to do get our data ready for TensorFlow. We need to convert the data from numpy arrays into Tensors. Fortunately for us the `Dataset` module has us covered. \n",
    "It provides a handy function, `from_tensor_slices` that creates the dataset to which we can then apply multiple transformations to shuffle, batch and repeat samples and plug into our training pipeline. Moreover, with just a few changes we could be loading the data from files on disk and the framework does all the memory management.\n",
    "We define two input functions: one for processing the training data and one for processing the test data. We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional `\"len\"` key to both that captures the length of the original, unpadded sequence, which we will use later.\n",
    "\"\"\"\n",
    "\n",
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x_train_variable))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\"\"\"### Baselines\n",
    "It's always a good practice to start any machine learning project trying out a couple of reliable baselines. Simple is always better and it is key to understand exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.\n",
    "With that in mind, let us start by trying out one of the simplest models out there for text classification. That is, a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. The fact that we don't care about the order of the words in the sentence is the reason why this method is generally known as a Bag-of-Words (BOW) approach. Let's see how that works out.\n",
    "We start out by defining the feature column that is used as input to our classifier. As we've seen [in this blog post](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html), `categorical_column_with_identity` is the right choice for this pre-processed text input. If we were feeding raw text tokens, other `feature_columns` could do a lot of the pre-processing for us. We can now use the pre-made `LinearClassifier`.\n",
    "\"\"\"\n",
    "\n",
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=[column], model_dir=os.path.join(model_dir, 'bow_sparse'))\n",
    "\n",
    "\"\"\"Finally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. Note that we do not aim to maximize performance in this blog post, so we only train our models for $25,000$ steps.\"\"\"\n",
    "\n",
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=25000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()\n",
    "#     # Un-comment code to download experiment data from Colaboratory\n",
    "#     from google.colab import files\n",
    "#     model_name = os.path.basename(os.path.normpath(classifier.model_dir))\n",
    "#     ! zip -r {model_name + '.zip'} {classifier.model_dir}\n",
    "#     files.download(model_name + '.zip')\n",
    "\n",
    "train_and_evaluate(classifier)\n",
    "\n",
    "\"\"\"One of the benefits of choosing a simple model is that it's much more inspectable. The more complex the model is, the more it tends to work like a black box. In this example we can load the weights from our model's last checkpoint and take a look at what tokens correspond to the  biggest weights in absolute value. The results looks like what we would expect\"\"\"\n",
    "\n",
    "weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()\n",
    "sorted_indexes = np.argsort(weights)\n",
    "extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))\n",
    "extreme_weights = sorted([(weights[i], word_inverted_index[i - index_offset]) for i in extremes])\n",
    "\n",
    "y_pos = np.arange(len(extreme_weights))\n",
    "plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Most significant tokens') \n",
    "plt.show()\n",
    "\n",
    "\"\"\"As we can see, tokens with the most positive weight such as 'refreshing' are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) scores.\n",
    "### Embeddings\n",
    "The next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space---when learned from a large enough corpus---has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an `embedding_column`. The representation seen by the model is the mean of the embeddings for each token (see the `combiner` argument in the [docs](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)). We can plug in the embedded features into a pre-canned `DNNClassifier`. \n",
    "A note for the keen observer: an `embedding_column` is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending on the chosen combiner. A direct consequence of this is that it wouldn't make sense to use an `embedding_column` directly in a `LinearClassifier` because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.\n",
    "\"\"\"\n",
    "\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=embedding_size)\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[100],\n",
    "    feature_columns=[word_embedding_column], \n",
    "    model_dir=os.path.join(model_dir, 'bow_embeddings'))\n",
    "train_and_evaluate(classifier)\n",
    "\n",
    "\"\"\"We can use TensorBoard to visualize our $50$-dimensional word vectors projected into $\\mathbb{R}^3$ using [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). We expect similar word to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviours. There's plenty of more information to go deeper [here](https://www.tensorflow.org/programmers_guide/embedding). The following snippet will generate a vocabulary file `metadata.tsv` that lists all the tokens in order. In the **PROJECTOR** tab in *TensorBoard* you can load it to visualize your vectors and there's also the [standalone projector visualizer](http://projector.tensorflow.org) that can be used to check out different embeddings.\n",
    "![Embedding image](https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/embeddings.gif?raw=true)\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(model_dir, 'metadata.tsv'), 'w', encoding=\"utf-8\") as f:\n",
    "    f.write('label\\n')\n",
    "    for index in range(-index_offset + 1, vocab_size - index_offset + 1):\n",
    "        f.write(word_inverted_index[index] + '\\n')\n",
    "\n",
    "\"\"\"### Convolutions\n",
    "At this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.\n",
    "Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for [image classification](https://www.tensorflow.org/tutorials/layers). The intuition is that certain sequences of words, or *n-grams*, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.\n",
    "### Creating a custom estimator\n",
    "The `tf.estimator` framework provides a higher level API for training machine learning models, defining `train()`, `evaluate()` and `predict()` operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. One the many benefits it provides is that the same code will be able to run in CPUs, GPUs and even in a distributed setup. There's a small family of pre-made estimators, like the ones we used earlier, but it's most likely that you will need to build your own. [This](https://www.tensorflow.org/extend/estimators) guide contains a thorough explanation on how to do it.\n",
    "We will use a `Head` to simplify the writing of our model function `model_fn`. The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. We will use `binary_classification_head`, which is a head for single label binary classification that uses `sigmoid_cross_entropy_with_logits` loss.\n",
    "The model presented here is a port from [this example](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py) into the `Estimator` API.\n",
    "\"\"\"\n",
    "\n",
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=params['embedding_initializer'])\n",
    "    \n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "                                    rate=0.2, \n",
    "                                    training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=dropout_emb,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    \n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    \n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "                                       rate=0.2, \n",
    "                                       training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)\n",
    "  \n",
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)\n",
    "\n",
    "train_and_evaluate(cnn_classifier)\n",
    "\n",
    "\"\"\"### LSTM Networks\n",
    "Using the `Estimator` API and the same model `head`, we can also create a classifier that uses a *Long Short-Term Memory* (*LSTM*) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.\n",
    "One of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models are deeper and more complex, which usually results in slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients. Having said that, with sufficient tuning they obtain state-of-the-art results for many problems. As a rule of thumb, CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.\n",
    "Each cell processes one token embedding at a time updating its internal state based on a differentiable computation that depends on both the embedding vector $x_t$ and the previous state $h_{t-1}$. In order to get a better understanding of how LSTMs work, you can refer to Chris Olahs [blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "![LSTM Architecture](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "<small><p align=\"center\">\n",
    "Source: <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a> by <strong>Chris Olah</strong>\n",
    "</p></small>\n",
    "In the beginning of this notebook, we padded all documents up to $200$ tokens, which is necessary to build a proper tensor. However, when a document contains fewer than $200$ words, we don't want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence's end. We can do this by using the `\"len\"` feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.\n",
    "We can use the same logic as above and simply need to replace the convolutional, pooling, and flatten layers with our LSTM cell.\n",
    "\"\"\"\n",
    "\n",
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def lstm_model_fn(features, labels, mode):    \n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    outputs = final_states.h\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "\n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                                         model_dir=os.path.join(model_dir, 'lstm'))\n",
    "train_and_evaluate(lstm_classifier)\n",
    "\n",
    "\"\"\"### Pretrained vectors\n",
    "Most of the models that we have shown before rely on word embeddings as a first layer, and we have so far initialized this embedding layer randomly, however it has been shown [in](https://arxiv.org/abs/1607.01759) [the](https://arxiv.org/abs/1301.3781) [literature](https://arxiv.org/abs/1103.0398), that especially for small labelled datasets, it is beneficial to train a pretrain word embeddings on a large unlabelled corpora using an unsupervised task. One such task is shown [here](https://www.tensorflow.org/tutorials/word2vec). This technique is an instance of *transfer learning*.\n",
    "To this end, we will show you how to use them in an `Estimator`. We will use the pre-trained vectors from another popular model, [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "We download the pretrained vectors and define a function that loads them into a `numpy.array`.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if not os.path.exists('glove.6B.zip'):\n",
    "    raise Exception('Please download glove data from http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "if not os.path.exists('glove.6B.50d.txt'):\n",
    "    raise Exception('Please unzip glove.6B.zip')\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            w = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[w] = vectors\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in word_index.items():\n",
    "        v = embeddings.get(w)\n",
    "        if v is not None and i < vocab_size:\n",
    "            embedding_matrix[i] = v\n",
    "            num_loaded += 1\n",
    "    print('Successfully loaded pretrained embeddings for '\n",
    "          f'{num_loaded}/{vocab_size} words.')\n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_glove_embeddings('glove.6B.50d.txt')\n",
    "\n",
    "\"\"\"To create a CNN classifier that leverages pretrained embeddings, we can reuse our `cnn_model_fn` but pass in a custom initializer that initializes the embeddings with our pretrained embedding matrix.\"\"\"\n",
    "\n",
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix\n",
    "\n",
    "params = {'embedding_initializer': my_initializer}\n",
    "cnn_pretrained_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn_pretrained'),\n",
    "                                        params=params)\n",
    "train_and_evaluate(cnn_pretrained_classifier)\n",
    "\n",
    "\"\"\"## Results\n",
    "### Launching TensorBoard\n",
    "Now we can launch TensorBoard and see how the different models we've trained compare against each other in terms of training time and performance.\n",
    "In a terminal, do\n",
    "```bash\n",
    "> tensorboard --logdir={model_dir}\n",
    "```\n",
    "We can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds.\n",
    "![PR curve](https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/pr_curves.png) \n",
    "![loss](https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/loss.png)\n",
    "### Getting Predictions\n",
    "To get predictions on new sentences we can use the `predict` method in the `Estimator` instances, which will load the latest checkpoint for each model and evaluate on the unseen examples. But before passing the data into the model we have to clean up, tokenize and map each token to the corresponding index, as shown here.\n",
    "It's worth noting that the checkpoint itelf is not enough to make predictions since the actual code used to build the estimator is necessary as well, in order to map the saved weights into the corresponding tensors, so it's a good practice associate saved checkpoints with the branch of code with which they were created.\n",
    "If your are interested in exporting the models to disk in a fully recoverable way you might want to look into the [SavedModel](https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators) class, specially useful for serving your model through an API using [TensorFlow Serving](https://github.com/tensorflow/serving).\n",
    "\"\"\"\n",
    "\n",
    "def text_to_index(sentence):\n",
    "    # Remove punctuation characters except for the apostrophe\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\"'\", ''))\n",
    "    tokens = sentence.translate(translator).lower().split()\n",
    "    return np.array([1] + [word_index[t] + index_offset if t in word_index else 2 for t in tokens])\n",
    "\n",
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               padding='post', \n",
    "                               value=0)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "            \n",
    "print_predictions([\n",
    "    'I really liked the movie!',\n",
    "    'Hated every second of it...'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
